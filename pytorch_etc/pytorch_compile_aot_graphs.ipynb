{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TORCH_COMPILE_DEBUG\"] = \"1\"  # Dumps files in `torch_compile_debug/`\n",
    "\n",
    "# Choose which logs to enable\n",
    "# os.environ[\"TORCH_LOGS\"] = \"+dynamo,+aot_graphs,+inductor,+guards,+graph\"\n",
    "os.environ[\"TORCH_LOGS\"] = \"+inductor,+aot_graphs\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch._dynamo import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple MLP model\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size=32, hidden_size=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleMLP()\n",
    "\n",
    "# Create Dummy Input\n",
    "input_data = torch.randn(1, 32)\n",
    "\n",
    "# 2. Enable TorchDynamo with TorchInductor Backend\n",
    "# We use torch.compile to enable the JIT compilation process.\n",
    "# Specifying backend='inductor' tells TorchDynamo to use TorchInductor\n",
    "# for the actual code generation.\n",
    "compiled_model = torch.compile(model, backend='inductor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-20 23:50:49,958] torch._logging._internal: [WARNING] Using TORCH_LOGS environment variable for log settings, ignoring call to set_logs\n",
      "[2024-12-20 23:50:49,981] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      "[2024-12-20 23:50:49,981] [0/0] torch._dynamo.output_graph.__graph: [DEBUG]  __compiled_fn_0 <eval_with_key>.0 opcode       name            target          args                 kwargs\n",
      "[2024-12-20 23:50:49,981] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] -----------  --------------  --------------  -------------------  --------\n",
      "[2024-12-20 23:50:49,981] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] placeholder  l_x_            L_x_            ()                   {}\n",
      "[2024-12-20 23:50:49,981] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] call_module  l__self___fc1   L__self___fc1   (l_x_,)              {}\n",
      "[2024-12-20 23:50:49,981] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] call_module  l__self___relu  L__self___relu  (l__self___fc1,)     {}\n",
      "[2024-12-20 23:50:49,981] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] call_module  l__self___fc2   L__self___fc2   (l__self___relu,)    {}\n",
      "[2024-12-20 23:50:49,981] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] output       output          output          ((l__self___fc2,),)  {}\n",
      "[2024-12-20 23:50:49,981] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] \n",
      "[2024-12-20 23:50:54,829] [0/0] torch._inductor.debug: [WARNING] model__0_forward_55 debug trace: /tmp/torchinductor_gaurav/25/c25kbzkukyctkk2zmf6cty6cntwki5nbfewl266wxncjwfsxgvoa.debug\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from the compiled model: tensor([[ 0.2117,  0.1899,  0.0066, -0.1359, -0.1234, -0.0568, -0.2092, -0.0407,\n",
      "         -0.0576,  0.1012]], grad_fn=<CompiledFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 3. Dynamic Graph Capture (TorchDynamo)\n",
    "# When you first call the compiled model with a specific input signature,\n",
    "# TorchDynamo intercepts the execution and dynamically builds a graph\n",
    "# representing the operations.\n",
    "# We can trigger this by running the model once. This first execution\n",
    "# will involve the graph capture.\n",
    "output = compiled_model(input_data)\n",
    "print(\"Output from the compiled model:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Python code from the aot_graph output\n",
    "\n",
    "Capture the logs generated by enabling `aot_graph` and feeding it into Gemini 2.0 Thinking model and ChatGPT.\n",
    "Then compare the code for the low-level ForwardGraph and BackwardGraph to make sure the capture code is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ForwardGraph(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Assuming the primals correspond to weights and biases\n",
    "        self.fc1_weight = nn.Parameter(torch.empty(64, 32))  # primals_1\n",
    "        self.fc1_bias = nn.Parameter(torch.empty(64))      # primals_2\n",
    "        self.fc2_weight = nn.Parameter(torch.empty(10, 64)) # primals_3\n",
    "        self.fc2_bias = nn.Parameter(torch.empty(10))      # primals_4\n",
    "\n",
    "    def forward(self, primals_5):  # Input to the forward pass\n",
    "        # File: /tmp/ipykernel_18358/2190141578.py:9, code: out = self.fc1(x)\n",
    "        permute = torch.permute(self.fc1_weight, [1, 0])\n",
    "        addmm = torch.addmm(self.fc1_bias, primals_5, permute)\n",
    "\n",
    "        # File: /tmp/ipykernel_18358/2190141578.py:10, code: out = self.relu(out)\n",
    "        relu = torch.relu(addmm)\n",
    "\n",
    "        # File: /tmp/ipykernel_18358/2190141578.py:11, code: out = self.fc2(out)\n",
    "        permute_1 = torch.permute(self.fc2_weight, [1, 0])\n",
    "        addmm_1 = torch.addmm(self.fc2_bias, relu, permute_1)\n",
    "        permute_2 = torch.permute(permute_1, [1, 0])\n",
    "\n",
    "        return addmm_1, primals_5, relu, permute_2\n",
    "\n",
    "class BackwardGraph(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, primals_5, relu, permute_2, tangents_1):\n",
    "        # File: /tmp/ipykernel_18358/2190141578.py:11, code: out = self.fc2(out)\n",
    "        mm = torch.mm(tangents_1, permute_2)\n",
    "        permute_3 = torch.permute(tangents_1, [1, 0])\n",
    "        mm_1 = torch.mm(permute_3, relu)\n",
    "        permute_4 = torch.permute(mm_1, [1, 0])\n",
    "        sum_1 = torch.sum(tangents_1, dim=[0], keepdim=True)\n",
    "        view = sum_1.view(10)  # Corrected line\n",
    "        permute_5 = torch.permute(permute_4, [1, 0])\n",
    "\n",
    "        # File: /tmp/ipykernel_18358/2190141578.py:10, code: out = self.relu(out)\n",
    "        le = torch.le(relu, 0)\n",
    "        full_default = torch.full([], 0.0, dtype=torch.float32, layout=torch.strided, device=tangents_1.device, pin_memory=False)\n",
    "        where = torch.where(le, full_default, mm)\n",
    "\n",
    "        # File: /tmp/ipykernel_18358/2190141578.py:9, code: out = self.fc1(x)\n",
    "        permute_6 = torch.permute(where, [1, 0])\n",
    "        mm_2 = torch.mm(permute_6, primals_5)\n",
    "        permute_7 = torch.permute(mm_2, [1, 0])\n",
    "        sum_2 = torch.sum(where, dim=[0], keepdim=True)\n",
    "        view_1 = sum_2.view(64)  # Corrected line\n",
    "        permute_8 = torch.permute(permute_7, [1, 0])\n",
    "\n",
    "        return permute_8, view_1, permute_5, view, None\n",
    "\n",
    "# Example Usage (assuming you have an input 'x'):\n",
    "model = ForwardGraph()\n",
    "# Initialize parameters (important for a real model)\n",
    "with torch.no_grad():\n",
    "    model.fc1_weight[:] = torch.randn_like(model.fc1_weight)\n",
    "    model.fc1_bias[:] = torch.randn_like(model.fc1_bias)\n",
    "    model.fc2_weight[:] = torch.randn_like(model.fc2_weight)\n",
    "    model.fc2_bias[:] = torch.randn_like(model.fc2_bias)\n",
    "\n",
    "x = torch.randn(1, 32)\n",
    "output, primals_5_saved, relu_saved, permute_2_saved = model(x)\n",
    "\n",
    "# Assume you have the gradients of the output with respect to the output (e.g., from a loss function)\n",
    "output_gradients = torch.randn_like(output)\n",
    "\n",
    "backward_graph = BackwardGraph()\n",
    "gradients = backward_graph(primals_5_saved, relu_saved, permute_2_saved, output_gradients)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass outputs match!\n"
     ]
    }
   ],
   "source": [
    "# Initialize models and input\n",
    "simple_model = SimpleMLP()\n",
    "low_level_forward = ForwardGraph()\n",
    "low_level_backward = BackwardGraph()\n",
    "\n",
    "# Initialize parameters consistently\n",
    "with torch.no_grad():\n",
    "    low_level_forward.fc1_weight[:] = simple_model.fc1.weight\n",
    "    low_level_forward.fc1_bias[:] = simple_model.fc1.bias\n",
    "    low_level_forward.fc2_weight[:] = simple_model.fc2.weight\n",
    "    low_level_forward.fc2_bias[:] = simple_model.fc2.bias\n",
    "\n",
    "input_tensor = torch.randn(1, 32, requires_grad=True)\n",
    "\n",
    "# 3. Compare Forward Pass Outputs\n",
    "output_simple = simple_model(input_tensor)\n",
    "output_simple.retain_grad()\n",
    "output_low_level, primals_5_saved, relu_saved, permute_2_saved = low_level_forward(input_tensor)\n",
    "assert torch.allclose(output_simple, output_low_level, atol=1e-5), \"Forward pass outputs do not match!\"\n",
    "print(\"Forward pass outputs match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2062, -0.2366,  0.1161, -0.0224, -0.1292, -0.2070, -0.1095,  0.0493,\n",
      "          0.2279,  0.3504]])\n"
     ]
    }
   ],
   "source": [
    "# 4. Compare Backward Pass Gradients\n",
    "# Define a simple loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "target = torch.randn_like(output_simple)\n",
    "loss_simple = loss_fn(output_simple, target)\n",
    "\n",
    "# Run backpropagation on the simple model\n",
    "simple_model.zero_grad()\n",
    "\n",
    "# If you need to perform multiple backward passes through the same graph \n",
    "# (e.g., for computing higher-order derivatives), you can retain the graph \n",
    "# by specifying retain_graph=True in the backward() call:\n",
    "loss_simple.backward(retain_graph=True)\n",
    "\n",
    "print(output_simple.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_fc1_weight_simple = torch.Size([64, 32])\n",
      "grad_fc1_weight_low_level = torch.Size([64, 32])\n",
      "grad_fc2_weight_simple = torch.Size([10, 64])\n",
      "grad_fc2_weight_low_level = torch.Size([10, 64])\n",
      "Backward pass gradients match!\n"
     ]
    }
   ],
   "source": [
    "# Get gradients from the simple model\n",
    "grad_fc1_weight_simple = simple_model.fc1.weight.grad\n",
    "grad_fc1_bias_simple = simple_model.fc1.bias.grad\n",
    "grad_fc2_weight_simple = simple_model.fc2.weight.grad\n",
    "grad_fc2_bias_simple = simple_model.fc2.bias.grad\n",
    "\n",
    "# Create output gradients for the low-level backward pass\n",
    "output_gradients_low_level = output_simple.grad\n",
    "\n",
    "# Run the low-level backward pass\n",
    "gradients_low_level = low_level_backward(primals_5_saved, relu_saved, permute_2_saved, output_gradients_low_level)\n",
    "grad_fc1_weight_low_level, grad_fc1_bias_low_level, grad_fc2_weight_low_level, grad_fc2_bias_low_level, _ = gradients_low_level\n",
    "\n",
    "print(f\"grad_fc1_weight_simple = {grad_fc1_weight_simple.shape}\")\n",
    "print(f\"grad_fc1_weight_low_level = {grad_fc1_weight_low_level.shape}\")\n",
    "print(f\"grad_fc2_weight_simple = {grad_fc2_weight_simple.shape}\")\n",
    "print(f\"grad_fc2_weight_low_level = {grad_fc2_weight_low_level.shape}\")\n",
    "\n",
    "# Compare gradients\n",
    "assert torch.allclose(grad_fc1_weight_simple, grad_fc1_weight_low_level, atol=1e-5), \"fc1 weight gradients do not match!\"\n",
    "assert torch.allclose(grad_fc1_bias_simple, grad_fc1_bias_low_level, atol=1e-5), \"fc1 bias gradients do not match!\"\n",
    "assert torch.allclose(grad_fc2_weight_simple.T, grad_fc2_weight_low_level.T, atol=1e-5), \"fc2 weight gradients do not match!\" # Note the transpose\n",
    "assert torch.allclose(grad_fc2_bias_simple, grad_fc2_bias_low_level, atol=1e-5), \"fc2 bias gradients do not match!\"\n",
    "\n",
    "print(\"Backward pass gradients match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
