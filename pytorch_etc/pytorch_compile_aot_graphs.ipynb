{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TORCH_COMPILE_DEBUG\"] = \"1\"  # Dumps files in `torch_compile_debug/`\n",
    "\n",
    "# Choose which logs to enable\n",
    "# os.environ[\"TORCH_LOGS\"] = \"+dynamo,+aot_graphs,+inductor,+guards,+graph\"\n",
    "os.environ[\"TORCH_LOGS\"] = \"+inductor,+aot_graphs\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch._dynamo import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple MLP model\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size=32, hidden_size=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleMLP()\n",
    "\n",
    "# Create Dummy Input\n",
    "input_data = torch.randn(1, 32)\n",
    "\n",
    "# 2. Enable TorchDynamo with TorchInductor Backend\n",
    "# We use torch.compile to enable the JIT compilation process.\n",
    "# Specifying backend='inductor' tells TorchDynamo to use TorchInductor\n",
    "# for the actual code generation.\n",
    "compiled_model = torch.compile(model, backend='inductor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-28 17:10:05,343] torch._logging._internal: [WARNING] Using TORCH_LOGS environment variable for log settings, ignoring call to set_logs\n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO] TRACED GRAPH\n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]  ===== Forward graph 0 =====\n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]  <eval_with_key>.158 class GraphModule(torch.nn.Module):\n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]     def forward(self, primals_1: f32[64, 32], primals_2: f32[64], primals_3: f32[10, 64], primals_4: f32[10], primals_5: f32[1, 32]):\n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         # File: /tmp/ipykernel_1629490/1870733671.py:10, code: out = self.fc1(x)\n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         permute: f32[32, 64] = torch.ops.aten.permute.default(primals_1, [1, 0]);  primals_1 = None\n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         addmm: f32[1, 64] = torch.ops.aten.addmm.default(primals_2, primals_5, permute);  primals_2 = permute = None\n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         \n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         # File: /tmp/ipykernel_1629490/1870733671.py:11, code: out = self.relu(out)\n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         relu: f32[1, 64] = torch.ops.aten.relu.default(addmm);  addmm = None\n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         \n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         # File: /tmp/ipykernel_1629490/1870733671.py:12, code: out = self.fc2(out)\n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         permute_1: f32[64, 10] = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None\n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         addmm_1: f32[1, 10] = torch.ops.aten.addmm.default(primals_4, relu, permute_1);  primals_4 = None\n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         permute_2: f32[10, 64] = torch.ops.aten.permute.default(permute_1, [1, 0]);  permute_1 = None\n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         return [addmm_1, primals_5, relu, permute_2]\n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         \n",
      "[2024-12-28 17:10:07,057] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO] \n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO] TRACED GRAPH\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]  ===== Backward graph 0 =====\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]  <eval_with_key>.159 class GraphModule(torch.nn.Module):\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]     def forward(self, primals_5: f32[1, 32], relu: f32[1, 64], permute_2: f32[10, 64], tangents_1: f32[1, 10]):\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         # File: /tmp/ipykernel_1629490/1870733671.py:12, code: out = self.fc2(out)\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         mm: f32[1, 64] = torch.ops.aten.mm.default(tangents_1, permute_2);  permute_2 = None\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         permute_3: f32[10, 1] = torch.ops.aten.permute.default(tangents_1, [1, 0])\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         mm_1: f32[10, 64] = torch.ops.aten.mm.default(permute_3, relu);  permute_3 = None\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         permute_4: f32[64, 10] = torch.ops.aten.permute.default(mm_1, [1, 0]);  mm_1 = None\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         sum_1: f32[1, 10] = torch.ops.aten.sum.dim_IntList(tangents_1, [0], True);  tangents_1 = None\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         view: f32[10] = torch.ops.aten.view.default(sum_1, [10]);  sum_1 = None\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         permute_5: f32[10, 64] = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         \n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         # File: /tmp/ipykernel_1629490/1870733671.py:11, code: out = self.relu(out)\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         le: b8[1, 64] = torch.ops.aten.le.Scalar(relu, 0);  relu = None\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         full_default: f32[] = torch.ops.aten.full.default([], 0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         where: f32[1, 64] = torch.ops.aten.where.self(le, full_default, mm);  le = full_default = mm = None\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         \n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         # File: /tmp/ipykernel_1629490/1870733671.py:10, code: out = self.fc1(x)\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         permute_6: f32[64, 1] = torch.ops.aten.permute.default(where, [1, 0])\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         mm_2: f32[64, 32] = torch.ops.aten.mm.default(permute_6, primals_5);  permute_6 = primals_5 = None\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         permute_7: f32[32, 64] = torch.ops.aten.permute.default(mm_2, [1, 0]);  mm_2 = None\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         sum_2: f32[1, 64] = torch.ops.aten.sum.dim_IntList(where, [0], True);  where = None\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         view_1: f32[64] = torch.ops.aten.view.default(sum_2, [64]);  sum_2 = None\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         permute_8: f32[64, 32] = torch.ops.aten.permute.default(permute_7, [1, 0]);  permute_7 = None\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         return [permute_8, view_1, permute_5, view, None]\n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO]         \n",
      "[2024-12-28 17:10:07,058] [0/0] torch._functorch.aot_autograd.__aot_graphs: [INFO] \n",
      "[2024-12-28 17:10:07,059] [0/0] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0\n",
      "[2024-12-28 17:10:07,096] [0/0] torch._inductor.graph: [DEBUG] Force channels last inputs for 0 conv for the current graph with id 0\n",
      "[2024-12-28 17:10:07,102] [0/0] torch._inductor.scheduler: [INFO] Number of scheduler nodes after fusion 3\n",
      "[2024-12-28 17:10:10,355] [0/0] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_gaurav/25/c25kbzkukyctkk2zmf6cty6cntwki5nbfewl266wxncjwfsxgvoa.py\n",
      "[2024-12-28 17:10:10,356] [0/0] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0\n",
      "[2024-12-28 17:10:10,357] [0/0] torch._inductor.debug: [WARNING] model__0_forward_55 debug trace: /tmp/torchinductor_gaurav/25/c25kbzkukyctkk2zmf6cty6cntwki5nbfewl266wxncjwfsxgvoa.debug\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from the compiled model: tensor([[ 0.0371,  0.1205,  0.4015,  0.2375,  0.0879,  0.0724,  0.0501,  0.2128,\n",
      "         -0.5271,  0.0094]], grad_fn=<CompiledFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 3. Dynamic Graph Capture (TorchDynamo)\n",
    "# When you first call the compiled model with a specific input signature,\n",
    "# TorchDynamo intercepts the execution and dynamically builds a graph\n",
    "# representing the operations.\n",
    "# We can trigger this by running the model once. This first execution\n",
    "# will involve the graph capture.\n",
    "output = compiled_model(input_data)\n",
    "print(\"Output from the compiled model:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Python code from the aot_graph output\n",
    "\n",
    "Capture the logs generated by enabling `aot_graph` and feeding it into Gemini 2.0 Thinking model and ChatGPT.\n",
    "Then compare the code for the low-level ForwardGraph and BackwardGraph to make sure the capture code is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ForwardGraph(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Assuming the primals correspond to weights and biases\n",
    "        self.fc1_weight = nn.Parameter(torch.empty(64, 32))  # primals_1\n",
    "        self.fc1_bias = nn.Parameter(torch.empty(64))      # primals_2\n",
    "        self.fc2_weight = nn.Parameter(torch.empty(10, 64)) # primals_3\n",
    "        self.fc2_bias = nn.Parameter(torch.empty(10))      # primals_4\n",
    "\n",
    "    def forward(self, primals_5):  # Input to the forward pass\n",
    "        # File: /tmp/ipykernel_18358/2190141578.py:9, code: out = self.fc1(x)\n",
    "        permute = torch.permute(self.fc1_weight, [1, 0])\n",
    "        addmm = torch.addmm(self.fc1_bias, primals_5, permute)\n",
    "\n",
    "        # File: /tmp/ipykernel_18358/2190141578.py:10, code: out = self.relu(out)\n",
    "        relu = torch.relu(addmm)\n",
    "\n",
    "        # File: /tmp/ipykernel_18358/2190141578.py:11, code: out = self.fc2(out)\n",
    "        permute_1 = torch.permute(self.fc2_weight, [1, 0])\n",
    "        addmm_1 = torch.addmm(self.fc2_bias, relu, permute_1)\n",
    "        permute_2 = torch.permute(permute_1, [1, 0])\n",
    "\n",
    "        return addmm_1, primals_5, relu, permute_2\n",
    "\n",
    "class BackwardGraph(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, primals_5, relu, permute_2, tangents_1):\n",
    "        # File: /tmp/ipykernel_18358/2190141578.py:11, code: out = self.fc2(out)\n",
    "        mm = torch.mm(tangents_1, permute_2)\n",
    "        permute_3 = torch.permute(tangents_1, [1, 0])\n",
    "        mm_1 = torch.mm(permute_3, relu)\n",
    "        permute_4 = torch.permute(mm_1, [1, 0])\n",
    "        sum_1 = torch.sum(tangents_1, dim=[0], keepdim=True)\n",
    "        view = sum_1.view(10)  # Corrected line\n",
    "        permute_5 = torch.permute(permute_4, [1, 0])\n",
    "\n",
    "        # File: /tmp/ipykernel_18358/2190141578.py:10, code: out = self.relu(out)\n",
    "        le = torch.le(relu, 0)\n",
    "        full_default = torch.full([], 0.0, dtype=torch.float32, layout=torch.strided, device=tangents_1.device, pin_memory=False)\n",
    "        where = torch.where(le, full_default, mm)\n",
    "\n",
    "        # File: /tmp/ipykernel_18358/2190141578.py:9, code: out = self.fc1(x)\n",
    "        permute_6 = torch.permute(where, [1, 0])\n",
    "        mm_2 = torch.mm(permute_6, primals_5)\n",
    "        permute_7 = torch.permute(mm_2, [1, 0])\n",
    "        sum_2 = torch.sum(where, dim=[0], keepdim=True)\n",
    "        view_1 = sum_2.view(64)  # Corrected line\n",
    "        permute_8 = torch.permute(permute_7, [1, 0])\n",
    "\n",
    "        return permute_8, view_1, permute_5, view, None\n",
    "\n",
    "# Example Usage (assuming you have an input 'x'):\n",
    "model = ForwardGraph()\n",
    "# Initialize parameters (important for a real model)\n",
    "with torch.no_grad():\n",
    "    model.fc1_weight[:] = torch.randn_like(model.fc1_weight)\n",
    "    model.fc1_bias[:] = torch.randn_like(model.fc1_bias)\n",
    "    model.fc2_weight[:] = torch.randn_like(model.fc2_weight)\n",
    "    model.fc2_bias[:] = torch.randn_like(model.fc2_bias)\n",
    "\n",
    "x = torch.randn(1, 32)\n",
    "output, primals_5_saved, relu_saved, permute_2_saved = model(x)\n",
    "\n",
    "# Assume you have the gradients of the output with respect to the output (e.g., from a loss function)\n",
    "output_gradients = torch.randn_like(output)\n",
    "\n",
    "backward_graph = BackwardGraph()\n",
    "gradients = backward_graph(primals_5_saved, relu_saved, permute_2_saved, output_gradients)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass outputs match!\n"
     ]
    }
   ],
   "source": [
    "# Initialize models and input\n",
    "simple_model = SimpleMLP()\n",
    "low_level_forward = ForwardGraph()\n",
    "low_level_backward = BackwardGraph()\n",
    "\n",
    "# Initialize parameters consistently\n",
    "with torch.no_grad():\n",
    "    low_level_forward.fc1_weight[:] = simple_model.fc1.weight\n",
    "    low_level_forward.fc1_bias[:] = simple_model.fc1.bias\n",
    "    low_level_forward.fc2_weight[:] = simple_model.fc2.weight\n",
    "    low_level_forward.fc2_bias[:] = simple_model.fc2.bias\n",
    "\n",
    "input_tensor = torch.randn(1, 32, requires_grad=True)\n",
    "\n",
    "# 3. Compare Forward Pass Outputs\n",
    "output_simple = simple_model(input_tensor)\n",
    "output_simple.retain_grad()\n",
    "output_low_level, primals_5_saved, relu_saved, permute_2_saved = low_level_forward(input_tensor)\n",
    "assert torch.allclose(output_simple, output_low_level, atol=1e-5), \"Forward pass outputs do not match!\"\n",
    "print(\"Forward pass outputs match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1635,  0.0598,  0.0341, -0.3071, -0.1186, -0.2797, -0.1256,  0.2093,\n",
      "          0.1464,  0.0386]])\n"
     ]
    }
   ],
   "source": [
    "# 4. Compare Backward Pass Gradients\n",
    "# Define a simple loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "target = torch.randn_like(output_simple)\n",
    "loss_simple = loss_fn(output_simple, target)\n",
    "\n",
    "# Run backpropagation on the simple model\n",
    "simple_model.zero_grad()\n",
    "\n",
    "# If you need to perform multiple backward passes through the same graph \n",
    "# (e.g., for computing higher-order derivatives), you can retain the graph \n",
    "# by specifying retain_graph=True in the backward() call:\n",
    "loss_simple.backward(retain_graph=True)\n",
    "\n",
    "print(output_simple.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_fc1_weight_simple = torch.Size([64, 32])\n",
      "grad_fc1_weight_low_level = torch.Size([64, 32])\n",
      "grad_fc2_weight_simple = torch.Size([10, 64])\n",
      "grad_fc2_weight_low_level = torch.Size([10, 64])\n",
      "Backward pass gradients match!\n"
     ]
    }
   ],
   "source": [
    "# Get gradients from the simple model\n",
    "grad_fc1_weight_simple = simple_model.fc1.weight.grad\n",
    "grad_fc1_bias_simple = simple_model.fc1.bias.grad\n",
    "grad_fc2_weight_simple = simple_model.fc2.weight.grad\n",
    "grad_fc2_bias_simple = simple_model.fc2.bias.grad\n",
    "\n",
    "# Create output gradients for the low-level backward pass\n",
    "output_gradients_low_level = output_simple.grad\n",
    "\n",
    "# Run the low-level backward pass\n",
    "gradients_low_level = low_level_backward(primals_5_saved, relu_saved, permute_2_saved, output_gradients_low_level)\n",
    "grad_fc1_weight_low_level, grad_fc1_bias_low_level, grad_fc2_weight_low_level, grad_fc2_bias_low_level, _ = gradients_low_level\n",
    "\n",
    "print(f\"grad_fc1_weight_simple = {grad_fc1_weight_simple.shape}\")\n",
    "print(f\"grad_fc1_weight_low_level = {grad_fc1_weight_low_level.shape}\")\n",
    "print(f\"grad_fc2_weight_simple = {grad_fc2_weight_simple.shape}\")\n",
    "print(f\"grad_fc2_weight_low_level = {grad_fc2_weight_low_level.shape}\")\n",
    "\n",
    "# Compare gradients\n",
    "assert torch.allclose(grad_fc1_weight_simple, grad_fc1_weight_low_level, atol=1e-5), \"fc1 weight gradients do not match!\"\n",
    "assert torch.allclose(grad_fc1_bias_simple, grad_fc1_bias_low_level, atol=1e-5), \"fc1 bias gradients do not match!\"\n",
    "assert torch.allclose(grad_fc2_weight_simple.T, grad_fc2_weight_low_level.T, atol=1e-5), \"fc2 weight gradients do not match!\" # Note the transpose\n",
    "assert torch.allclose(grad_fc2_bias_simple, grad_fc2_bias_low_level, atol=1e-5), \"fc2 bias gradients do not match!\"\n",
    "\n",
    "print(\"Backward pass gradients match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
